{"cells":[{"cell_type":"markdown","source":"# WestCoastAD Optimization Package","metadata":{"tags":[],"cell_id":"00000-2a7493d0-a6e9-4bef-985a-a138bf60f579","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 1. Introduction \n\nOptimization theories and methods have been applied in many fields to solve various practical problems.\nGiven the advances in computational systems, optimization techniques have become increasingly important and popular in different engineering applications. \nOptimization is one of the core content of machine learning. \nThe essence of most machine learning algorithms is to build an optimization model and learn the optimal parameters of the objective function from the given data.\n\n\nUnconstrained scalar function optimization problems can be solved exactly by setting the derivative to zero and solving the resulting equation. \nHowever, we can't always solve the resulting equation especially when dealing with more complex higher-dimensional problems \nbecause directly computing the solution is hard and often impossible. \nThus, we have to use iterative optimization methods such as gradient descent, \nwhich heavily relies on calculating the derivative of the objective function. \nGradient plays an essential role in machine learning.\nNeural networks can gradually increase accuracy with every training session through the process of gradient descent (Wang, 2019). \nHowever, for many problems, calculating derivatives analytically is both time-consuming and computationally impractical. \nThe ability to efficiently evaluate derivatives is essential in optimization systems.\n\nThere are three main methods of calculating derivatives: \nnumerical differentiation, symbolic differentiation, and automatic differentiation. \nSpecifically, symbolic differentiation involves computing exact expressions for the derivatives in terms of \nthe function variables by representing the expression as a tree and manipulating it using the rules of differentiation. \nThe derivatives can quickly become very complicated for complex functions and higher-order derivatives. \nThus, for complicated functions, symbolic differentiation is not easily generalizable and becomes computationally inefficient \ndue to redundant evaluations of different components of the function. Numerical differentiation aims to approximate derivatives \nthrough finite differentiating, but the solution accuracy is greatly affected by the truncation and round-off errors \nassociated with finite difference formulas.\n\nAutomatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. \nIt can give exact answers in constant time(Saroufim, 2019), \nwhich is a powerful tool to automate the calculation of derivatives, \nespecially when differentiating complex algorithms and mathematical functions(Margossian, 2019). \nAutomatic differentiation is more computationally efficient and more generalizable than \nsymbolic differentiation and can find exact derivatives as opposed to numerical differentiation. \n\nIn WestCoastAD Optimization Package, we implement several common optimization methods with a \nforward mode automatic differentiation core to compute gradients. \nIn Section 2, we describe the mathematical background of some common optimization methods and the concepts of automatic \ndifferentiation. In Section 3, you can find the installation instruction and usage demos. \nIn Sections 4 to 5, we describe the  organization and implementation of classes and functiones in WestCoastAD. \nWe then discuss the broader impact and inclusicity statement in Section 6 and lastly our future plan to \nextend the package in Section 7.\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00000-f3df3825-7567-4479-b99f-c1e70305c317","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 2. Background\n\n## 2.1 Optimization Algorithms \n\nThere are various iterative optimization algorithms. All these algorithms start with an initial guess for the optimal inputs to the optimization function and iteratively update this guess by a small amount using some type of information about that function. Once a certain stopping condition is met (for instance a minimum value constraint on the update size or a maximum constraint on the number of iterations), the algorithm will terminate and return its last estimate of the optimal inputs to the function and the optimal value of the function. \n\nWhile these algorithms can converge to the optima of convex functions, they are not guaranteed to find the global optima of non-convex functions and are susceptible to convergence to local optima based on their initialization. Despite this, iterative optimization functions are ubiquitous in machine learning as finding local optima can still be useful. Below, we will go over various optimization algorithms that have been implemented in WestCoastAD. In the context of WestCoastAD, an optimization problem refers to the minimization of a function so we will be using these two terms interchangeably.\n\n### 2.1.1 Gradient Descent\n\nOptimization algorithms that belong to this category iteratively update the function inputs by subtracting the gradient of the optimization function with respect to these variables at the current estimate for the variables. This is because mathematically, the negative of the gradient points in the direction of steepest descent. Thus by greedily moving in the steepest local direction, we are guaranteed to improve our estimations at every iteration. \n\n$$\nx_{t+1} = x_{t} - \\alpha \\nabla f(x_{t})\n$$\n\nThe $\\alpha$ parameter above is referred to as the learning rate of the algorithm and specifies how large a step the algorithm should take in each iteration.\n\nSeveral variations of the gradient descent update have been proposed in order to help improve its convergence and the speed of the algorithm.\n\n#### 2.1.1.1 Momentum Gradient Descent\n\nWhile the gradient locally points in the direction of steepest descent, it does not necessarily point along the shortest path to the function's minimum. Often times gradient descent will oscillate around the shortest path. Momentum gradient descent helps speed up the algorithm by using an exponential moving average of the gradients at each iteration thus making the updates better align with the shortest path to the minimum:\n\n$$\nx_{t+1} = x_{t} - \\alpha m_t\n$$\n\n$$\nm_t = \\beta m_{t-1} + (1-\\beta) \\nabla f(x_{t})\n$$\n\nWhere $\\beta$ is a user specified parameter commonly set to 0.9.\n\n#### 2.1.1.2 AdaGrad\n\nAs gradient descent gets closer to a minimum, taking smaller steps can help us converge closer to the minimum. However, if we set the learning rate too small from the beginning, the algorithm will take too long to converge. AdaGrad gradually decreases the learning rate over time to help with convergence without slowing down the algorithm.\n\n$$\nx_{t+1} = x_{t} - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} m_t\n$$\n\n$$\nv_t = v_{t-1} + (\\nabla f(x_{t}))^2\n$$\n\nWhere $\\epsilon$ is a small number added to avoid division by zero.\n\n#### 2.1.1.3 RMSprop\n\nSimilar to AdaGrad, this variation of gradient descent also tries to achieve a gradual decrease in the learning rate over time to help with convergence.\n\n$$\nx_{t+1} = x_{t} - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} m_t\n$$\n\n$$\nv_t = \\beta v_{t-1} + (1- \\beta) (\\nabla f(x_{t}))^2\n$$\n\nWhere $\\epsilon$ is a small number added to avoid division by zero and $\\beta$ is a user specified parameter commonly set to 0.9.\n\n#### 2.1.1.4 Adam\n\nAdam combines RMSprop and Momentum gradient descent in order to achieve the desireable traits of both variations.\n\n$$\nx_{t+1} = x_{t} - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} m_t\n$$\n\n$$\nm_t = \\frac{n_t}{1- \\beta_1^t}\n$$\n\n$$\nv_t = \\frac{w_t}{1- \\beta_2^t}\n$$\n\n$$\nn_t = \\beta_1 n_{t-1} + (1- \\beta_1) (\\nabla f(x_{t}))^2\n$$\n\n$$\nw_t = \\beta_2 w_{t-1} + (1- \\beta_2) (\\nabla f(x_{t}))^2\n$$\n\nWhere $\\epsilon$ is a small number added to avoid division by zero and $\\beta_1 = 0.9$ and $\\beta_2=0.999$.\n\n### 2.1.2 Quasi-Newton Methods\n\nNewton's method of optimization involves the computation of gradients and hessians. Hessians are very expensive to compute for functions with several input variables. Quasi-Newton methods forego hessian computations by starting with an initial guess for the Hessian and using gradient information to update this approximation at every iteration along with the optimal variable estimates. Thus Quasi-Newton algorithms complete the following steps in every iteration:\n\n1. Solve $B_t \\Delta x_{t} = - \\nabla f(x_t)$\n\n2. Update $x_{t+1}$ with $\\Delta x_{t}$\n\n3. Update $B_{t+1}$ based on $B_t$\n\nWhere $B$ is an approximation of the hessian. Different Quasi-Newton methods vary in how they update $B$.\n\n#### 2.1.2.1 BFGS\n\nBFGS uses a rank two update in step 3 which is given by: \n\n$$\nB_{t+1}^{-1} = \\left( I - \\frac{ s_t y_t^T }{ y_t^T s_t } \\right) B_t^{-1} \\left( I - \\frac{ y_t s_t^T }{ y_t^T s_t } \\right) + \\frac{ s_t s_t^T }{ y_t^T s_t }\n$$\n\nWhere $s_t = \\alpha \\Delta x_{t}$ and $y_t = \\nabla f(x_{t+1}) - \\nabla f(x_t)$\n\n## 2.2 Automatic Differentiation\n\nAs can be seen, all the aforementioned methods require derivative computations. Automatic differentiation is an efficient way of computing these derivatives which can be used with various complex functions. Automatic differentiation assumes that we are working with a differentiable function composed of a finite number of elementary functions and operations with known symbolic derivatives. The table below shows some examples of elementary functions and their respective derivatives:\n\n| Elementary Function    | Derivative              |\n| :--------------------: | :----------------------:|\n| $x^3$                  | $3 x^{2}$               | \n| $e^x$                  | $e^x$                   |\n| $\\sin(x)$              | $\\cos(x)$               |\n| $\\ln(x)$               | $\\frac{1}{x}$           |\n\nGiven a list of elementary functions and their corresponding derivatives, the automatic differentiation process involves the evaluations of the derivatives of complex compositions of these elementary functions through repeated applications of the chain rule:\n\n$$\n \\frac{\\partial}{\\partial x}\\left[f_1 \\circ (f_2 \\circ \\ldots (f_{n-1} \\circ f_n)) \\right] = \n\\frac{\\partial f_1}{\\partial f_2} \\frac{\\partial f_2}{\\partial f_3} \\ldots \\frac{\\partial f_{n-1}}{\\partial f_n}\\frac{\\partial f_n}{\\partial x}\n$$\n\nThis process can be applied to the evaluation of partial derivatives as well thus allowing for computing derivatives of multivariate and vector-valued functions.\n\n### 2.2.1 Computational Graph\n\nThe forward mode automatic differentiation process described above can be visualized in a computational graph, a directed graph with each node corresponding to the result of an elementary operation.\n\nFor example, consider the simple problem of evaluating the following function and its derivative at $x=2$:\n$$\nf(x) = x^3 +2x^2\n$$\nThe evaluation of this function can be represented by the evaluation trace and computational graph below where the numbers in orange are the function values and the numbers in blue are the derivatives evaluated after applying each elementary operation:\n\n<img src=\"https://raw.githubusercontent.com/anita76/playground/master/src/ex_comp_graph.png\" width=\"75%\" />\n\n| Trace       | Elementary Function     | Value   | Derivative                | Derivative Value      |\n| :---------: | :----------------------:| :------:| :------------------------:| :--------------------:|\n| $v_1$       | $x$                     | $2$     | $\\dot{x}$                 | $1$                   |\n| $v_2$       | $v_1^2$                 | $4$     | $2v_1 \\dot{v}_1$          | $4$                   |\n| $v_3$       | $v_1^3$                 | $8$     | $3v_1^2 \\dot{v}_1$        | $12$                  |\n| $v_4$       | $2v_2$                  | $8$     | $2 \\dot{v}_2$             | $8$                   |\n| $v_5$       | $v_3 + v_4$             | $16$    | $ \\dot{v}_3 + \\dot{v}_4$  | $20$                  |\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00001-722a3bee-8936-4ee5-9976-f665f5c41e37","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 3. How to Use WestCoastAD\n\n### 3.1 Installation with GitHub\n\n1. Clone the package repository:\n\n    `git clone https://github.com/West-Coast-Differentiators/cs107-FinalProject.git`\n\n\n2. Create and activate a new environment:\n    \n    * Method 1: (Note: If you do not have Anaconda installed, follow the instructions on their [website](https://deepnote.com/project/07e48b43-3d96-4960-83bf-911527fb55ea?cellId=00002-de6540a6-d222-4d8b-b063-a392b8a9b7d4#/milestone2-2.ipynb) to install it.)\n \n        * Create a new conda environment:\n        \n        `conda create --name <env_name> python=3.8` \n\n        * Activate your virtual environment:\n\n        `conda activate <env_name>`\n    \n    * Method 2: \n        * Install pip environment package\n\n        `pip install virtualenv` \n    \n        * Create a new pip environment:\n        \n        `virtualenv <env_name>`\n\n        * Activate your virtual environment:\n\n        Mac OS / Linux: `source env_name/bin/activate`\n\n        Windows: `env_name\\Scripts\\activate`\n\n3. Navigate to the base repository:\n\n    `cd cs107-FinalProject`\n\n4. Install the package and its requirements:\n\n    `pip install ./`\n\n5. You may check that the installation was successful by running the package tests:\n\n    `python -m unittest discover -s WestCoastAD/test -p '*_test.py'`\n\n6. To deactivate the virtual environment\n\n    `conda deactivate` (or `deactivate` for pip virtualenv)\n\n### 3.2 Installation with pip\n\n1. Create and activate a new environment:\n    * Method 1: (Note: If you do not have Anaconda installed, follow the instructions on their [website](https://deepnote.com/project/07e48b43-3d96-4960-83bf-911527fb55ea?cellId=00002-de6540a6-d222-4d8b-b063-a392b8a9b7d4#/milestone2-2.ipynb) to install it.)\n    \n        * Create a new conda environment:\n        \n        `conda create --name <env_name> python=3.8` \n\n        * Activate your virtual environment:\n\n        `conda activate <env_name>`\n    \n    * Method 2: \n        * Install pip environment package\n\n        `pip install virtualenv` \n    \n        * Create a new pip environment:\n        \n        `virtualenv <env_name>`\n\n        * Activate your virtual environment:\n\n        Mac OS / Linux: `source env_name/bin/activate`\n\n        Windows: `env_name\\Scripts\\activate`\n        \n2. Install the package and its requirements:\n\n    `pip install westcoastAD`\n\n3. To deactivate the virtual environment\n\n    `conda deactivate` (or `deactivate` for pip virtualenv)\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00002-994189a2-274a-4015-90e1-867afb1b9997","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 3.3 Demos\n\n#### 3.3.1 Optimization \n\n##### 3.3.1.1 Minimizing a univariate scalar function:","metadata":{"tags":[],"cell_id":"00004-6f0cb096-ce3d-4c3a-aa9e-23d14e291936","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# import all the necessary libraries\n>>> import numpy as np\n>>> import WestCoastAD as ad\n\n# define the function that you wish to optimize. Your function can only include elementary\n# operations that are supported by WestCoastAD. Please refer to the Software Implementation\n# section of this documentation for a complete list.\n\n>>> def my_objective_f(x):\n>>>    return x*x.log(base=2)\n\n# create an optimizer class with your objective function and an array of initial values for\n# the inputs to the function that you want to optimize.\n\n# You may use one of the initializer classes provided by WestCoastAD\n>>> initialization = ad.RandomNormal(mean=1,stddev=0.2)(shape=1) \n\n# Or initialize the values yourself by creating a 1D numpy array of the right size\n>>> initialization = np.array([2])\n\n>>> optimizer = ad.Optimizer(my_objective_f, initialization)\n\n# You can call any of the optimization algorithms provided by WestCoastAD on your optimizer\n# You may need to try different parameter settings in order for the algorithm to converge.\n# The first returned value is the best value of the function and the second value is an array\n# containing the best values for the inputs to the function.\n>>> optimizer.gd_optimize(num_iterations=1000, learning_rate=0.01)\n(-0.530737845423043, array([0.36787944]))\n\n>>> optimizer.adam_optimize(num_iterations=1000, learning_rate=0.01)\n(-0.530737845423043, array([0.36787944]))\n\n# you may access the history of the optimization function values to check for convergence.\n# The history will include the progression of the optimization function values over the\n# algorithm iterations for the last optimization algorithm that you ran.\nhistory = optimizer.val_history\n","metadata":{"tags":[],"cell_id":"00004-20790356-cbb1-4b9d-84ee-b75ba0fd2b29","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ##### 3.3.1.2 Minimizing a multivariate scalar function:","metadata":{"tags":[],"cell_id":"00005-48636600-7cb4-4767-9c0e-7cabe15243bf","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":">>> import numpy as np\n>>> import WestCoastAD as ad\n\n# You may define multivariate scalar functions as a function of multiple\n# scalar inputs\n\n>>> def my_objective_f1(x, y, z):\n>>>    return (x-2)**2*y**2 + (1-z)**2\n\n# Or you may define them as a function with a single vector input \n>>> def my_objective_f2(x):\n>>>    return (x[0]-2)**2*x[1]**2 + (1-x[2])**2\n\n# create an optimizer class with your objective function and an array of initial values for\n# the inputs to the function that you want to optimize. The size of the initalization array\n# has to match the size of the input vector or the number of inputs to your function if your\n# functions takes scalar inputs.\n>>> initialization = np.array([3, 5, -1])\n\n# When creating an optimizer, you must set the scalar parameter to False if your\n# objective function takes a vector as input.\n>>> optimizer = ad.Optimizer(my_objective_f1, initialization)\n>>> optimizer = ad.Optimizer(my_objective_f2, initialization, scalar=False)\n\n>>> optimizer.bfgs_optimize(num_iterations=100000, learning_rate=0.001)\n(6.460561661947661e-26, array([2.00000000e+00, 1.55279742e-05, 1.00000000e+00]))","metadata":{"tags":[],"cell_id":"00007-794ca063-0c69-483c-9649-54c673746339","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 3.3.2 Automatic Differentiation\n\nYou may also use WestCoastAD to differentiate multivariate and univariate scalar and vector functions.\n\n##### 3.3.2.1 Differentiating a univariate scalar function","metadata":{"tags":[],"cell_id":"00007-aaa0760e-061c-40a7-b5e9-1647a2b2362f","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# import all the necessary libraries\n>>> import numpy as np\n>>> import WestCoastAD as ad\n\n# define the function that you wish to differentiate. Your function can only include elementary\n# operations that are supported by WestCoastAD. Please refer to the Software Implementation\n# section of this documentation for a complete list.\n\n>>> def my_func(x):\n>>>    return x.sin() + x.cos()\n\n# You can use WestCoastAD's differentiate function to evaluate your function value and derivative\n# you must provide differentiate with a 1D array containing the values of the inputs to your function.\n# The first returned value is the function value and the returned array contains the derivative value\n>>> x_val = np.array([np.pi/2])\n>>> ad.differentiate(my_func, variable_values=x_val)\n(1.0, array([-1.]))","metadata":{"tags":[],"cell_id":"00008-561c84bb-5e4f-4140-860c-ff3b9aff97dd","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Alternatively, you can use WestCoastAD's Variable class to write the function that you want to differentiate. This allows you to experiment with different derivative seeds.","metadata":{"tags":[],"cell_id":"00009-69ccdc9c-08f6-4587-8cf8-162ce464cc0e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":true,"source_hash":null,"execution_start":1603314710130,"execution_millis":237,"cell_id":"00003-142b93f9-29f2-4f95-a687-aa3852f9fa77","deepnote_cell_type":"code"},"source":">>> from WestCoastAD import Variable\n>>> import numpy as np\n\n# define the variables of your function as WestCoastAD variables with the value at which you \n# want to evaluate the derivative and  a derivative seed value\n>>> x = Variable(value=np.pi/2, derivative_seed=1)\n\n# define your function in terms of the WestCoastAD variable objects\n>>> f = x.sin() + np.cos(x)\n>>> print(f.value)\n1.0\n>>> print(f.derivative)\n-0.9999999999","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.3.2.2 Differentiating a multivariate scalar function:","metadata":{"tags":[],"cell_id":"00011-c750f6bd-d05c-4071-b03b-621e36020a58","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":">>> import numpy as np\n>>> import WestCoastAD as ad\n\n# You may define multivariate scalar functions as a function of multiple\n# scalar inputs\n\n>>> def my_func1(x, y, z):\n>>>    return (x-2)**2*y**2 + (1-z)**2\n\n# Or you may define them as a function with a single vector input \n>>> def my_func2(x):\n>>>    return (x[0]-2)**2*x[1]**2 + (1-x[2])**2\n\n# The array below specifies the values x=3, y=0, and z=4 which will be given to the differentiate\n# function\n>>> variable_values = np.array([1, 0, 4])\n\n# When calling the differentiate function, you must set the scalar parameter to False if your\n# objective function takes a vector as input.\n# The returned array contains the gradient with respect to x, y, and z in this order\n>>> ad.differentiate(my_func1, variable_values)\n(9, array([0., 0., 6.]))\n>>> ad.differentiate(my_func2, variable_values, scalar=False)\n(9, array([0., 0., 6.]))","metadata":{"tags":[],"cell_id":"00012-a21c4a3e-ccf9-46a2-b977-b033d849b105","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Alternatively, you can use WestCoastAD's Variable class to write the function that you want to differentiate. This allows you to experiment with different derivative seeds.","metadata":{"tags":[],"cell_id":"00013-3e6800af-2d82-40ad-8103-114f884c16aa","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":">>> from WestCoastAD import Variable, multivariate_dimension_check\n>>> import numpy as np\n\n# define the variables of your function as WestCoastAD variables with the value at which you \n# want to evaluate the derivative and  a derivative seed value. Your variables must have the\n# derivative seeds with the same dimensionality. We've used a dimensionality of 3 in this\n# example since there are 3 variables\n>>> x = Variable(value=1, derivative_seed=np.array([1, 0, 0]))\n>>> y = Variable(value=0, derivative_seed=np.array([0, 1, 0]))\n>>> z = Variable(value=4, derivative_seed=np.array([0, 0, 1]))\n\n# you may use WestCoastAD to check that your variable functions have equal dimensionalities\n>>> multivariate_dimension_check([x, y, z])\nTrue\n\n# define your function in terms of the WestCzzzoastAD variable objects\n>>> f = (x-2)**2*y**2 + (1-z)**2\n>>> print(f.value)\n9\n>>> print(f.derivative)\n[0., 0., 6.]","metadata":{"tags":[],"cell_id":"00014-327795b1-a845-4352-8f48-9d43e1099bf9","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### 3.3.2.3 Differentiating a univariate vector function:","metadata":{"tags":[],"cell_id":"00015-9b57c86d-ea9d-4ca2-91be-2eef15fe5def","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":">>> import numpy as np\n>>> import WestCoastAD as ad\n\n# To define a vector function, create a python function that returns the \n# vector function as a 1D numpy array\n\n>>> def my_func(x):\n>>>    return np.array([x*2, x, x.log(base=2)])\n\n# Provide the function and its variable value to differentiate \n>>> value, derivative = ad.differentiate(my_func, variable_values=np.array([2]))\n\n# The value will be returned as a 1D array.\n>>> print(value)\n[4. 2. 1.]\n\n# The derivative will be returned as a 2D array.\n>>> print(derivative)\n[[2.        ]\n [1.        ]\n [0.72134752]]","metadata":{"tags":[],"cell_id":"00016-65cd5e83-2194-40f9-b4f0-3890c684ccf5","output_cleared":true,"source_hash":null,"execution_start":1607745147476,"execution_millis":36,"deepnote_cell_type":"code"},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Alternatively, you can use WestCoastAD's Variable class to write the function that you want to differentiate. This allows you to experiment with different derivative seeds.","metadata":{"tags":[],"cell_id":"00017-af3010a5-3a7c-4530-83cf-849e7d4ad5b7","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":">>> from WestCoastAD import Variable, vector_function_jacobian, vector_function_value\n\n# define the variable of your function as a WestCoastAD variable\n>>> x = Variable(value=2, derivative_seed=1)\n\n# define your function as a numpy array in terms of the WestCoastAD variable object\n>>> f = np.array([x*2, x, x.log(base=2)])\n\n# you can obtain the value of your vector function using vector_function_value\n>>> vector_function_value(f)\narray([4., 2., 1.])\n\n# you can obtain the Jacobian of your vector function using vector_function_jacobian\n>>> vector_function_jacobian(f)\narray([[2.        ],\n       [1.        ],\n       [0.72134752]])","metadata":{"tags":[],"cell_id":"00018-c1bba3ef-89d0-427a-8db3-cf443d122436","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### 3.3.2.4 Differentiating a multivariate vector function:","metadata":{"tags":[],"cell_id":"00019-7428e2e5-4751-4967-9823-2ec6b02c32d3","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":">>> import numpy as np\n>>> import WestCoastAD as ad\n\n# To define a vector function, create a python function that returns the \n# vector function as a 1D numpy array\n\n>>> def my_func1(x, y):\n>>>    return np.array([x*y, x-y, x.log(base=2)])\n\n# You may also define your vector function to take a single vector as input\n>>> def my_func2(x):\n>>>    return np.array([x[0]*x[1], x[0]-x[1], x[0].log(base=2)])\n\n# Provide the function and its variable values to differentiate \n# The example below sets x=1 and y=3\n>>> value, derivative = ad.differentiate(my_func1, variable_values=np.array([1, 3]))\n\n# if your function takes a vector as input, you must set the scalar parameter to False\n>>> value, derivative = ad.differentiate(my_func2, variable_values=np.array([1, 3]), scalar=False)\n\n# The value will be returned as a 1D array.\n>>> print(value)\n[ 3. -2.  0.]\n\n# The derivative will be returned as a 2D array. The value at (0, 0) is the derivative of\n# f_0 with respect to x, the value at (0, 1) is the derivative of f_0 with respect to y,\n# the value at (1, 0) is the derivative of f_1 with respect to x and so on.\n>>> print(derivative)\n[[ 3.          1.        ]\n [ 1.         -1.        ]\n [ 1.44269504  0.        ]]","metadata":{"tags":[],"cell_id":"00020-72f182e0-6c4e-4b65-a507-80de40de19a3","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Alternatively, you can use WestCoastAD's Variable class to write the function that you want to differentiate. This allows you to experiment with different derivative seeds.","metadata":{"tags":[],"cell_id":"00021-2d0e6d5b-b8d1-45aa-92f9-8983de3a806f","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":">>> from WestCoastAD import Variable, vector_function_jacobian, vector_function_value\n>>> import numpy as np\n\n# define the variables of your function as WestCoastAD variables\n>>> x = Variable(value=1, derivative_seed=np.array([1, 0]))\n>>> y = Variable(value=3, derivative_seed=np.array([0, 1]))\n\n# define your function as a numpy array in terms of the WestCoastAD variable objects\n>>> f = np.array([x*y, x-y, x.log(base=2)])\n\n# you can obtain the value of your vector function using vector_function_value\n>>> vector_function_value(f)\narray([ 3., -2.,  0.])\n\n# you can obtain the Jacobian of your vector function using vector_function_jacobian\n>>> vector_function_jacobian(f)\narray([[ 3.        ,  1.        ],\n       [ 1.        , -1.        ],\n       [ 1.44269504,  0.        ]])","metadata":{"tags":[],"cell_id":"00022-151d1a3e-7e7e-4375-9750-b14561e1d3a3","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Software Organization\n## 4.1 Directory Structure\n```bash\ncs107-FinalProject  \n             setup.py\n             .travis.yml \n             README.MD\n             .gitignore\n             docs/\n                             documentation.ipynb\n                             milestone1.ipynb\n                             milestone2.ipynb   \n                             milestone2A.ipynb\n                             milestone2_progress.ipynb\n                             \n             WestCoastAD/  \n                             __init__.py\n                             src/    \n                                     __init__.py\n                                     forward_mode_AD.py\n                                     forward_mode_AD_auxiliaries.py\n                                     initializer.py\n                                     optimizer.py\n                                     \n                             tests/ \n                                     __init__.py\n                                     forward_mode_AD_test.py \n                                     forward_mode_AD_auxiliaries_test.py\n                                     initializer_test.py\n                                     optimizer_test.py\n                                     \n                     \n```\n## 4.2 Modules and Basic Functionality\n\n#### Docs Module\n* This module contains the documentation for the package. The documentation include the information about the package including the purpose, the background information about Optimization and Auto Differentiation, the organization of the software, how to use the package and how to install the package.\n\n#### Setup Module\n* This module contains configurations to allow users to install our package. Inside the setup.py file, we use the setuptools library for packaging the modules and installing the package dependencies. Our package requires that numpy be installed. We do not use a requirements.txt for this library; all the dependencies are handled through setup.py.\n\n#### Forward Mode Module\n* This module includes the implementation of the forward mode of automatic differentiation. The module defines a Variable class that will take the value to be evaluated and the derivative seed value. The derivative and value of functions defined in terms of Variable objects can be accessed by viewing the `derivative` and `value` attributes of this class. Each method in the Variable class, contains extensive documentation about the function and doctests that provide examples. \n\n#### Optimization Module\n* This is the extension module that was included in the final release of our package. As we developed the final release, the optimization feature become the main feature of our software. At the core of our optimization functions was automatic differentiation which allowed us to create various algorithms that are prevalent in machine learning applications. For each method in the Optimizer class, we have provided documentation about the function and doctests that provide examples. The software implementation section of the documentation below will describe each implementation in detail.\n\n#### Forward Mode AD Auxiliaries Module\n* This module was created to expand AD to handle vector functions with multiple real scalar or vector inputs. The module contains several methods that expand the functionality of the Variable class. A detailed explanation of the methods is in the software implementation section. All functions have extension documentation and doctest that provide examples.\n\n#### Initializer Module\n* This module was created as a way to help user create the required parameters for the various optimization methods. A detailed explanation of the methods is in the extension section. All functions have documentation and doctests that provide examples. \n\n#### Test Modules\n* This module contains all the tests of our implementation including unit tests and acceptance tests. We used Python's unittest to define the tests and checked our coverage with CodeCov. To have access to the test modules, the github repo has to be cloned and made available locally. Following the instruction in the \"How to Use WestCoastAD\" section of this documentation, the tests can be run using the following command:\n`python -m unittest discover -s WestCoastAD/test -p '*_test.py'`. Alternatively, you can run individual test files using: `python WestCoastAD/test/<TEST_FILE_NAME>.py`\n\n## 4.3 Packaging and Build Pipeline\n\n#### Packaging\n* We used sdist and bdist_wheel to create distribution archives. Most modern pip versions use bdist_wheel but we included an sdist archive as a fall back. These were uploaded to PYPI with the use of Twine. Within our setup.py, we used setuptools following the standard python packaging protocols as described in the following tutorial: [https://python-packaging-tutorial.readthedocs.io/en/latest/setup_py.html](https://python-packaging-tutorial.readthedocs.io/en/latest/setup_py.html)\n\n#### Distribution\n* WestCoastAD is currently distributed on Github and the package has been uploaded to the package index PyPI. In the \"How to Use WestCoastAD\" section of this document, there is information about how to install a virtual environment, how to install the package itself directly from Github or through a pip install and if installed via github where to get the package.\n\n#### Travis CI\n* Travis CI allows for automatic building and testing of commits that are pushed to remote. It requires that all pull request be tested and if all the tests are passed, it will update the build automatically. Travis CI also contains a badge that will inform the user if the package is working or if it is down. The travis.yml file is located in our main directory.\n\n#### CodeCov\n* CodeCov provides details about our test coverage. This allows us to identify where tests need to be added and will help us discover possible bugs in the code. CodeCov also contains a badge that will inform the user of the percentage code coverage of our tests.\n\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00004-1eaafddd-4ef5-4d13-9c23-3ca11d55859d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 5. Software Implementation","metadata":{"tags":[],"output_cleared":false,"cell_id":"00005-e97e3185-732e-45ce-9622-70a4b24ae624","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"This section covers the class and functions for `WestCoastAD`.\n\n## 5.1 Automatic Differentiation\n\n`Variable` is the base class used to instantiate gradient objects and to perform derivatives.\nThe instances of the `Variable` class should be used to express the scalar or vector function to be differentiated.\n\n| Class Attributes        | Usage        |\n| :----------------------- |:-------------|\n| `value`                 | Value of the variable (`int` or `float`). The setter and getter for value are defined using the  `property` decorator.|\n| `derivative`            | Derivative of the function (`int`, `float`, or 1D `numpy` array). A `seed derivative` can be provided at the time of instantiation. This attribute is set and retrieved using a `property` decorator.|\n\n<br>\n\n**Class Methods**\n\n###### `__add__`\nDunder method for overloading the \"+\" operator. This method computes the value and the derivative of the summation operation on `int`, `float` & `Variable` instances. This method returns a new `Variable` object.\n\n###### `__radd__`\nSame method as `__add__` with reversed operands.\n\n###### `__mul__`\nDunder method for overloading the \"*\" operator. This method computes the value and the derivative following the [product rule](https://en.wikipedia.org/wiki/Product_rule) on `int`, `float` & `Variable` instances. It returns a new `Variable` object.\n\n###### `__rmul__`\nSame method as `__mul__` with reversed operands.\n\n###### `__sub__`\nDunder method for overloading the \"-\" operator. This method computes the value and the derivative of the substraction operation on `int`, `float` & `Variable` instances. It returns a new `Variable` object.\n\n###### `__rsub__`\nSame method as `__sub__` with reversed operands.\n\n###### `__truediv__`\nDunder method for overloading the \"/\" operator. This method computes the value and the derivative following the [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule) on `int`, `float` & `Variable` instances. It returns a new `Variable` object. This method throws a `ZeroDivisionError` if the divisor is 0.\n\n###### `__rtruediv__`\nSame method as `__truediv__` with reversed operands.\n\n###### `__pow__`\nDunder method for overloading the \"**\" operator. This method computes the value and the derivative following the [power rule](https://en.wikipedia.org/wiki/Power_rule) if exponent is `int` or `float`.\nIt returns a new `Variable` object. This method throws a `ValueError` if:\n* a negative valued variable/function is raised to a non-integer power (e.g. $x^{1.2}, x=-2$)\n* a zero valued variable/function is raised to a non-positive power (e.g. $x^{-5}, x = 0$)\n* a non-positive valued variable/function is raised to the power of another variable/function (e.g. $x^{sin(x)}, x=-1$). This is because the generalized power rule of differentiation takes the logarithm of the base and the logarithm is not defined for non-positive values.\n\n###### `__rpow__`\nDunder method which handles the differentiation of equations of the form $a^{f(x)}$. It returns a new `Variable` object. This method throws a `ValueError` if:\n* zero is raised to the power of a variable/function with a non-positive value (e.g. $0^{x}, x=0$). \n* a negative number is used as the base (e.g. $(-2)^x$)\n\n###### `__neg__`\nDunder method for overloading the negation operation. This method computes the value and derivative of the negation operation and returns a new `Variable` object.\n\n###### `__abs__`\nDunder method for overloading the abs function. This method computes the value and derivative of the absolute value of a variable and returns a `Variable` object. It will throw a `ValueError` if given a value of zero since the derivative of absolute value is not defined at zero.\n\n###### `__repr__`\nDunder method for overloading the `repr` function. Returns the object representation of the class `Variable` in the form `Variable(value=value, derivative=derivative)`\n\n###### `log`\nThis method computes the value and derivative of `log` function and returns a new `Variable` object. A `ValueError` is thrown for non-positive values.\n\n###### `exp`\nThis method computes the value and derivative of `exp` function and returns a new `Variable` object\n\n###### `sqrt`\nSquare root is a special case of pow and therefore calls `__pow__` with exponent `1\\2`.\n\n###### `sin`\n\nThis method computes the value and the derivative of the sine function. It returns a new `Variable` object.\n\n###### `cos`\n\nThis method computes the value and the derivative of the cosine function. It returns a new `Variable` object.\n\n###### `tan`\n\nThis method computes the value and the derivative of the tangent function. It returns a new `Variable` object. This method will throw a `ValueError` if given a value that is an odd multiple of $\\frac{\\pi}{2}$\n\n###### `sinh`\n\nThis method computes the value and the derivative of the hyperbolic sine function. It returns a new `Variable` object.\n\n###### `cosh`\n\nThis method computes the value and the derivative of the hyperbolic cosine function.  It returns a new `Variable` object.\n\n###### `tanh`\n\nThis method computes the value and the derivative of the hyperbolic tangent function.  It returns a new `Variable` object.\n\n###### `arcsin`\n\nThis method computes the value and the derivative of the arcsin function.  It should be given a value in  $[-1,1]$ for the derivative to be defined, otherwise a `ValueError` will be raised. It returns a new `Variable` object.\n\n###### `arccos`\n\nThis method computes the value and the derivative of the arccos function. It should be given a value in  $[-1,1]$ for the derivative to be defined, otherwise a `ValueError` will be raised. It returns a new `Variable` object.\n\n###### `arctan`\n\nThis method computes the value and the derivative of the arctan function. It returns a new `Variable` object.\n\n###### `__abs__`\n\nDunder method for overloading the abs function. Computes the value and derivative of abs function\n\n###### `__lt__`\n\nDunder method for overloading the less than comparison.\nThis operand will perform elementwise comparison of the value and derivative of self and other. It will return a tuple of booleans; the first element is True if the value of self is less than  the value of other and the second value is True if ALL the componenets of the derivative of self are (element-wise) less than the derivative of other. Thus this method can only be used to compare Variable objects of the same dimensionality.\n\n###### `__le__`\n\nDunder method for overloading the less than or equal to comparison.\nThis operand will perform elementwise comparison of the value and derivative of self and other.\nIt will return a tuple of booleans; the first element is True if the value of self is less than or equal to the value of other and the second value is True if ALL the componenets of the derivative of self are (element-wise) less than or equal to the derivative of other. Thus this method can only be used to compare Variable objects of the same dimensionality.\n\n###### `__eq__`\n\nDunder method for overloading the equal to comparison. This operand will perform elementwise comparison of the value and derivative of self and other.\nIt will return a tuple of booleans; the first element is True if the value of self is equal to the value of other and the second value is True if ALL the componenets of the derivative of self are (element-wise) equal to the derivative of other. Thus this method can only be used to compare Variable objects of the same dimensionality.\n\n###### `__ne__`\n\nDunder method for overloading the not equal to comparison.\nThis operand will perform elementwise comparison of the value and derivative of self and other.\nIt will return a tuple of booleans; the first element is True if the value of self is not equal to the value of other and \nthe second value is True if ANY components of the derivative of self are (element-wise) not equal to the derivative of other.\nThus this method can only be used to compare Variable objects of the same dimensionality.\n\n###### `__gt__`\n\nDunder method for overloading the greater than comparison.\nThis operand will perform elementwise comparison of the value and derivative of self and other.\nIt will return a tuple of booleans; the first element is True if the value of self is greater than the value of other and \nthe second value is True if ALL components of the derivative of self are (element-wise) greater than the derivative of other.\nThus this method can only be used to compare Variable objects of the same dimensionality.\n\n###### `__ge__`\n\nDunder method for overloading the greater than or equal to comparison.\nThis operand will perform elementwise comparison of the value and derivative of self and other.\nIt will return a tuple of booleans; the first element is True if the value of self is greater than or equal to the value of other and \nthe second value is True if ALL components of the derivative of self are (element-wise) greater than or equal to the derivative of other.\nThus this method can only be used to compare Variable objects of the same dimensionality.\n\n\n###### `logistic`\n\nThis method computes the value and derivative of the standard logistic function of the form $1/(1 + e^{-x})$\n\n#### Vector valued and multivariable functions \n\nWestcoastAD will also be able to work with functions that have a single vector (numpy array) as input. To achieve this capability, we have created a forward mode AD auxiliaries module. This module contains various method that can be used to compute values, derivatives, gradients, and Jacobians  of multi and univariate scalar and vector functions with scalar or vector inputs. These methods abstract out the creation of Variable objects thus making it easier for users to differentiate functions. Below is a description of each method.\n\n- **`vector_function_jacobian`** - this function takes a function and its variable values as a inputs and returns the jacobian of the given function as a single numpy array. \nThe input `scalar` should be set to False if the function is defined to take a single vector as input.\n\n- **`vector_function_value`** - this function takes a function and its variable values as a inputs and returns the value of the given function as a single numpy array.\nThe input `scalar` should be set to False if the function is defined to take a single vector as input.\n\n- **`multivariate_dimension_check`** - this function takes an numpy array of WestCoastAD variable instances and checks if the derivatives of the Variable objects have the same derivative dimensionality. If the dimensions are the same it will return True, else False.\n\n- **`differentiate`** - This function can be used to differentiate scalar and vector functions of one or more variables. It takes the function and its variable values as inputs. \nThe input `scalar` should be set to False if the function is defined to take a single vector as input.\n\n#### External Dependencies\n\nWe use `numpy` to evaluate elementary functions which were not available as part of the Python\nstandard library (e.g. sin, cos, etc). We use it to initialize vector inputs for multivariate functions.\nIt is also used to compute gradients and Jacobians of multivariable and vector-valued functions.\n`numpy.testing` is used for unit testing gradients. \n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00006-97e3629c-bc97-4be2-8a0b-028ce0e8c18b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 5.2 Optimization (our extension)\n\n### `Initializer` Module  \n\nThis module was created to help the user initialize the Optimizer class. The module contains various classes that will allow the user to generate different variable initializations for the objective function that is passed to the Optimizer class. The classes are listed below and a description is provided. \n\n* `Initializer Class` \n\nThis is the abstract base class, It contains the `__call__` dunder methods that will be overloaded in the other classes and a `get_config` method that is also overloaded in the subclasses. The `get_config` method when overloaded will display the parameters for that class. The `__call__` dunder method will return an array with the desired output for each class. This class cannot be used for initialization as it has no underlying implementation.\n\n* `Zeros  Class` \n\nThis will generate a numpy array of zeros. The user has to specify the size of the array and the class will generate an array of that size containing only zeros.\n\n* `Ones Class` \n\nThis will generate a numpy array of ones. The user has to specify the size of the array and the class will generate an array of that size containing only ones.\n\n* `Constant Class` \n\nThis will generate a numpy array of a specified value. The user will provide a constant value and the number of values that should be in the array. The user-provided information will be loaded into the get_config dictionary.\n\n* `RandomUniform Class`\n\nThis will generate a numpy array of random values based on a uniform distribution. The user will enter a minimum value, maximum value and the number of values that should be in the array. The user-provided information will be loaded into the get_config dictionary.\n\n* `RandomNormal Class` \n\nThis will generate a numpy array of random values based on a normal distribution. The user will provide a mean, standard deviation and the number of values that should be in the array. The user-provided information will be loaded into the get_config dictionary.\n\n\n### `Optimizer` Module\n\nThis class provides methods to perform optimization with gradient descent on objective functions built using the `Variable` class from `WestCoastAD`. `Optimizer` class has 6 different optimization algorithms.\n\n| Class Attributes        | Usage        |\n| :----------------------- |:-------------|\n| `objective_function`                 | A python function that takes as input a single vector or one or more scalars and returns a 1D numpy array of functions if objective_function is a vector function or a single function if objective_function is a scalar function.The function should be expressed using instances of the `Variable` class.|\n| `variable_initialization`            | a 1D numpy array of floats/ints containing initial values for the inputs to the objective function.|\n| `scalar`             |A boolean which is `True` if the inputs to objective_function are one or more scalars otherwise False.|\n| `verbose`            | A boolean specifying whether updates about the optimization process will be printed to the console.|\n| `val_history`            | A list which captures history of the optimization function values to check for convergence|\n\n\n<br>\n\n**Class Methods**\n\n**`gd_optimize(num_iterations=100, learning_rate=0.01, tolerance=None)`**\n\nThis method returns the minimum value of the objective function alongside the critical point \nusing regular gradient descent algorithm.\n\n*Parameters:*\n\n- `num_iterations` - An int specifying maximum # of iterations of gradient descent.\n\n- `learning_rate` - A float/int specifying the learning rate for gradient descent.\n\n- `tolerance` - A float specifying the smallest tolerance for the updates to the variables. If the L2 norm of the update step is smaller than this value, gradient descent will terminate.\n\n*Returns:*\n\n- A tuple with minimum value of the objective function and a scalar/vector input values at which the minima occured.\n\n\n**`momentum_optimize(num_iterations=100, learning_rate=0.01, beta=0.9, tolerance=None)`**\n\nThis method returns the minimum value of the objective function alongside the critical point using momentum based gradient descent learning.\nUsage of momentum in gradient descent to minimize loss function is commonly used in machine learning. Our implementation uses the `derivative` attribute of the objective function written using `Variable` class to obtain gradients in each iteration.\nAn exponential moving average of the current and the past gradients are used to determine the minima.\n\n*Parameters:*\n\n- `num_iterations` - An int specifying maximum # of iterations of gradient descent.\n\n- `learning_rate` - A float/int specifying the learning rate for gradient descent.\n\n- `beta` - A float ranging between 0 and 1 specifying the sample weight for exponential average of weights. The recommended value for this parameter is 0.9\n\n- `tolerance` - A float specifying the smallest tolerance for the updates to the variables. If the L2 norm of the update step is smaller than this value, gradient descent will terminate.\n\n*Returns:*\n\n- A tuple with minimum value of the objective function and a scalar/vector input values at which the minima occured.\n\n**`adagrad_optimize(num_iterations=1000, learning_rate=0.01, epsilon=1e-7, tolerance=None)`**\n\nThis method returns the minimum value of the objective function alongside the critical point using adaptive gradient (Adagrad) algorithm. Adagrad adjusts the learning rate alpha by dividing it by the square root of the cumulative sum of current and past squared gradients. Please refer to the background section for more details.\n\n*Parameters:*\n\n- `num_iterations` - An int specifying maximum # of iterations of gradient descent.\n\n- `learning_rate` - A float/int specifying the learning rate for gradient descent.\n\n- `epsilon` - A float to prevent division by zero during optimization.\n\n- `tolerance` - A float specifying the smallest tolerance for the updates to the variables. If the L2 norm of the update step is smaller than this value, gradient descent will terminate.\n\n*Returns:*\n\n- A tuple with minimum value of the objective function and a scalar/vector input values at which the minima occured.\n\n**`rmsprop_optimize(num_iterations=1000, learning_rate=0.001, epsilon=1e-7, beta=0.9, tolerance=None)`**\n\nThis method performs RMSProp gradient descent optimization on the objective function (which is expressed using instances of the `Variable` class). This is an enhanced version of the Adagrad optimizers. It adjusts the learning rate alpha by dividing it by the exponential moving averages of gradients.\n\n*Parameters:*\n\n- `num_iterations` - An int specifying maximum # of iterations of gradient descent.\n\n- `learning_rate` - A float/int specifying the learning rate for gradient descent.\n\n- `epsilon` - A float to prevent division by zero during optimization.\n\n- `beta` - A float ranging between 0 and 1 specifying the sample weight for exponential average of weights. The recommended value for this parameter is 0.9\n\n- `tolerance` - A float specifying the smallest tolerance for the updates to the variables. If the L2 norm of the update step is smaller than this value, gradient descent will terminate.\n\n*Returns:*\n\n- A tuple with minimum value of the objective function and a scalar/vector input values at which the minima occured.\n\n**`adam_optimize(num_iterations=1000, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, tolerance=None):`**\n\nThis method performs Adaptive Moment Estimation (ADAM) optimization on the objective function.\n\n*Parameters:*\n\n- `num_iterations` - An int specifying maximum # of iterations of gradient descent.\n\n- `learning_rate` - A float/int specifying the learning rate for gradient descent.\n\n- `beta1` - Exponential decay hyperparameter for the first moment estimates.\n\n- `beta2` - Exponential decay hyperparameter for the second moment estimates.\n\n- `epsilon` - A float to prevent division by zero during optimization.\n\n- `tolerance` - A float specifying the smallest tolerance for the updates to the variables. If the L2 norm of the update step is smaller than this value, gradient descent will terminate.\n\n*Returns:*\n\n- A tuple with minimum value of the objective function and a scalar/vector input values at which the minima occured.\n\n**`bfgs_optimize(num_iterations=1000, learning_rate=0.01, tolerance=None)`**\n\nThis method performs Quasi-Newton optimization of the objective function with BFGS updates.\n\n*Parameters:*\n\n- `num_iterations` - An int specifying maximum # of iterations of gradient descent.\n\n- `learning_rate` - A float/int specifying the learning rate for gradient descent.\n\n- `tolerance` - A float specifying the smallest tolerance for the updates to the variables. If the L2 norm of the update step is smaller than this value, gradient descent will terminate.\n\n*Returns:*\n\n- A tuple with minimum value of the objective function and a scalar/vector input values at which the minima occured.\n","metadata":{"tags":[],"cell_id":"00008-91a5a98d-b62b-4f80-990d-7b3da936520b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 6. Broader Impact and Inclusivity Statement\n\n### 6.1 Broader Impact\n\nOptimizers have a wide range of applications across various industries. Some of the use cases include \ncost/revenue optimization, optimal labor staffing at factories, automated location navigation, supply chain optimization, work allocation in crowd-sourcing markets.\n`WestCoastAD` enables automatic optimization of real word problems, modeled as objective functions for optimal decision making.\nThere is more work to be done as elaborated in Futures Section but what we have currently is a step in the right direction.\n\nBased on the usage context, optimization errors and lack of understanding of how optimizers work can affect societies in varying degrees.\nFor instance, inaccurate optimizations can result in delays/increased inventory costs in supply chain management. Knowledge about how an optimizer does optimization can have positive and negative impacts.\nPer study elaborated in [6], lack of transparency in algorithmic management affected the morale of employees.\nOn the contrary, there were scenarios where knowledge of how the algorithm optimized the objective encouraged offenders to game the system.\n\nIt is important to design optimizers in `WestCoastAD` that converge faster with high accuracy. There is the potential misuse of showcasing these methods as methods that can find optimal results but in reality they are very sensitive to parameter settings and unless the optimization function is convex, there is no guarantee that the algorithm will be close to the true global optimum.\n### 6.2 Software Inclusivity\n\nWe plan to release this software to the open source community for contribution.\nThe name `WestCoastAD` was something we came up with for the purposes of this class (since the core contributors were all on the Pacific time zone).\nPrior to open sourcing, we will rename our package to make it location neutral.\nWe will also rename repo `master` branch to `main` per [Github guidelines](https://github.com/github/renaming).\nWe will hold the contributors and peer reviews accountable to a code review standard.\nGoogle has published it's [engineering code review](https://github.com/google/eng-practices/blob/master/review/index.md) practice which we can adopt.\nWe welcome all members of the software community to contribute to this package. \nIn addition to functionality extension, there are opportunities to localize the software to make it accessible for contributors across all regions.\nWe would translate the documentation to multiple languages. \n","metadata":{"tags":[],"cell_id":"00014-b4a098b5-3620-46c9-88ce-339660f7182f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 7. Future Plan","metadata":{"tags":[],"cell_id":"00011-016ca835-323e-48ca-8a42-d816aa6846cf","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"An automatic-differentiation-based optimization package can be applied in many fields to handle various practical problems. In our future extensions, we mainly focus on improving the efficiency of gradient computation in machine learning models.\n\n### 7.1 Reverse mode/Backpropagation class\nAs we mentioned at the beginning, optimization packages are heavily used in deep learning. Training of a neural network is basically an optimization problem about finding the minimum of the loss function with respect to its set of weights. We will extend our class by adding reverse mode of automatic differentiation to accommodate back propagation. Reverse mode AD is more efficient at getting gradients of multivariate scalar functions of large dimensionality thus making it more suitable for neural networks. \n\n### 7.2 Common machine learning models class\nCommon machine learning models like linear regression and neural networks utilize optimization methods to train the model. We would like to extend our package to create classes for some common machine learning models in order to make it easier for users to utilize our optimizer for machine learning problems. The fit methods will be realized using our automatic-differentiation-based optimization class. \n\n### 7.3 Natural language processing models class\nNatural language processing(NLP) is the main area of deep learning, with common tasks in syntactic analysis, machine translation, language modelling, and etc. However, training an NLP model usually is time-consuming. Having the ability to quickly train the models in NLP is very important. In Yu and Siskind(2013)'s work on sentence tracking, automatic differentiation based gradient descent is proved to save the training time dramatically in C language. We may extend our software to implement NLP models based on our AD backed gradient descent based optimizers. To achieve this, we will need to tackle efficiency issues. Most importantly, we need to support automatic differentiation for matrix and vector operations (e.g. vector norms, matrix multiplication) in order to avoid using for loops and delegate time consuming mathematical operations to lower level languages such as C as done in packages like numpy.\n","metadata":{"tags":[],"cell_id":"00012-abe826a8-3821-45fc-a9e0-7a23ace12f48","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 8. References\n1.  Margossian, C. C. (2019). A review of automatic differentiation and its efficient implementation. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(4), 1–32. https://doi.org/10.1002/widm.1305\n\n2. Saroufim, M. (2019, November 13). Automatic Differentiation Step by Step. Retrieved October 15, 2020, from https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6\n\n3. Wang, C. (2019, March 3). Automatic Differentiation, Explained. Retrieved October15, 2020,from https://towardsdatascience.com/automatic-differentiation-explained-\tb4ba8e60c2ad\n\n4. Automatic Differentiation Background. Retrived October 20, 2020, from https://www.mathworks.com/help/deeplearning/ug/deep-learning-with-automatic-differentiation-in-matlab.html\n\n5. Haonan Yu and Jeffrey Mark Siskind. Grounded language learning from video described with sentences. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–63, Sofia, Bulgaria, 2013. Association for Computational\nLinguistics.\n\n6. Min Kyung Lee, Daniel Kusbit, Evan Metsky and Laura Dabbish. Working with Machines: The Impact of Algorithmic\nand Data-Driven Management on Human Workers. Retrieved December11, 2020 from https://dl.acm.org/doi/10.1145/2702123.2702548","metadata":{"tags":[],"output_cleared":false,"cell_id":"00009-9192a65f-7c58-4285-b43d-cae92e055796","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"output_cleared":false,"cell_id":"00010-9151e0c9-58e1-4c4b-8e67-fb88309d6a5e","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_notebook_id":"2d77ee3c-eec0-470c-abf3-e93f24c897bb","deepnote_execution_queue":[]}}